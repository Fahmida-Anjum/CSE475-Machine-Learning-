This project evaluates various ensemble learning techniques, including Bagging (Random Forest), Boosting (AdaBoost and Gradient Boosting), Stacking (using two base models and a meta-model), and Voting (Hard and Soft), to classify an imbalanced dataset. Each method was assessed using cross-validation and metrics such as accuracy, precision, recall, F1 score, and ROC-AUC. Random Forest achieved a balanced performance with an accuracy of 85.65%, while Soft Voting showed the best recall (25.82%) and Gradient Boosting demonstrated high precision (60.97%). Explainable AI tools like SHAP and LIME provided feature importance visualizations and instance-level explanations, revealing key factors influencing predictions. The analysis highlights trade-offs in accuracy, interpretability, and sensitivity across methods, showcasing the potential of ensemble learning to address imbalanced classification challenges effectively.
